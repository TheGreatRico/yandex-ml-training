{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание №7\n",
    "\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача №1: \n",
    "Обратимся к классической задаче распознавания рукописных цифр. Мы будем работать с набором данных [MNIST](http://yann.lecun.com/exdb/mnist/). В данном задании воспользуемся всем датасетом целиком.\n",
    "\n",
    "__Ваша основная задача: реализовать весь пайплан обучения модели и добиться качества $\\geq 92\\%$ на тестовой выборке.__\n",
    "\n",
    "Код для обучения модели в данном задании отсутствует. Присутствует лишь несколько тестов, которые помогут вам отладить свое решение. За примером можно обратиться к ноутбуку первого занятия.\n",
    "\n",
    "Настоятельно рекомендуем написать код \"с нуля\", лишь поглядывая на готовые примеры, а не просто \"скопировать-вставить\". Это поможет вам в дальнейшем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to .\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:01<00:00, 9619729.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\train-images-idx3-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to .\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 14536864.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\train-labels-idx1-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to .\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 4767978.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to .\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4678420.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to .\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Image label: 0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "train_mnist_data = MNIST('.', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_mnist_data = MNIST('.', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_mnist_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_mnist_data,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "random_batch = next(iter(train_data_loader))\n",
    "_image, _label = random_batch[0][0], random_batch[1][0]\n",
    "plt.figure()\n",
    "plt.imshow(_image.reshape(28, 28))\n",
    "plt.title(f'Image label: {_label}')\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте модель ниже. Пожалуйста, не стройте переусложненную сеть, не стоит делать ее глубже четырех слоев (можно и меньше). Ваша основная задача – обучить модель и получить качество на отложенной (тестовой выборке) не менее 92% accuracy.\n",
    "\n",
    "*Комментарий: для этого достаточно линейных слоев и функций активации.*\n",
    "\n",
    "__Внимание, ваша модель должна быть представлена именно переменной `model`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "class MnistNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(28**2,100)\n",
    "        self.Matrix2 = nn.Linear(100,50)\n",
    "        self.Matrix3 = nn.Linear(50,10)\n",
    "        self.R = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,28**2)\n",
    "        x = self.R(self.Matrix1(x))\n",
    "        x = self.R(self.Matrix2(x))\n",
    "        x = self.Matrix3(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "# Creating model instance\n",
    "model = MnistNeuralNet() # your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Локальные тесты для проверки вашей модели доступны ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything seems fine!\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert model is not None, 'Please, use `model` variable to store your model'\n",
    "\n",
    "try:\n",
    "    x = random_batch[0].reshape(-1, 784)\n",
    "    y = random_batch[1]\n",
    "\n",
    "    # compute outputs given inputs, both are variables\n",
    "    y_predicted = model(x)    \n",
    "except Exception as e:\n",
    "    print('Something is wrong with the model')\n",
    "    raise e\n",
    "    \n",
    "    \n",
    "assert y_predicted.shape[-1] == 10, 'Model should predict 10 logits/probas'\n",
    "\n",
    "print('Everything seems fine!')\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настройте параметры модели на обучающей выборке. Рекомендуем поработать с различными оптимизаторами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "real_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in train_data_loader:\n",
    "        y_predicted = model(batch[0].reshape(-1, 784))\n",
    "        predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "        real_labels.append(batch[1])\n",
    "\n",
    "predicted_labels = torch.cat(predicted_labels)\n",
    "real_labels = torch.cat(real_labels)\n",
    "train_acc = (predicted_labels == real_labels).type(torch.FloatTensor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Neural network accuracy on train set: {train_acc:3.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "real_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_data_loader:\n",
    "        y_predicted = model(batch[0].reshape(-1, 784))\n",
    "        predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "        real_labels.append(batch[1])\n",
    "\n",
    "predicted_labels = torch.cat(predicted_labels)\n",
    "real_labels = torch.cat(real_labels)\n",
    "test_acc = (predicted_labels == real_labels).type(torch.FloatTensor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Neural network accuracy on test set: {test_acc:3.5}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка, что необходимые пороги пройдены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_acc >= 0.92, 'Test accuracy is below 0.92 threshold'\n",
    "assert train_acc >= 0.91, 'Train accuracy is below 0.91 while test accuracy is fine. We recommend to check your model and data flow'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сдача задания\n",
    "Загрузите файл `hw07_data_dict.npy` (ссылка есть на странице с заданием) и запустите код ниже для генерации посылки. Код ниже может его загрузить (но в случае возникновения ошибки скачайте и загрузите его вручную)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/23s_dd_ml/homeworks/hw07_mnist_classification/hw07_data_dict.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.6074: 100%|██████████| 938/938 [00:09<00:00, 102.86it/s]\n",
      "val_loss: 0.0021, val_accuracy: 0.9581: 100%|██████████| 157/157 [00:00<00:00, 235.91it/s]\n",
      "Epoch 2, loss: 0.3013: 100%|██████████| 938/938 [00:09<00:00, 95.38it/s] \n",
      "val_loss: 0.0014, val_accuracy: 0.9690: 100%|██████████| 157/157 [00:00<00:00, 249.72it/s]\n",
      "Epoch 3, loss: 0.2455: 100%|██████████| 938/938 [00:08<00:00, 104.88it/s]\n",
      "val_loss: 0.0011, val_accuracy: 0.9763: 100%|██████████| 157/157 [00:00<00:00, 235.91it/s]\n",
      "Epoch 4, loss: 0.2231: 100%|██████████| 938/938 [00:09<00:00, 101.55it/s]\n",
      "val_loss: 0.0010, val_accuracy: 0.9792: 100%|██████████| 157/157 [00:00<00:00, 213.90it/s]\n",
      "Epoch 5, loss: 0.1982: 100%|██████████| 938/938 [00:08<00:00, 106.13it/s]\n",
      "val_loss: 0.0009, val_accuracy: 0.9802: 100%|██████████| 157/157 [00:00<00:00, 244.06it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests, gzip, os, hashlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from torchsummary import summary\n",
    "\n",
    "from model import Net\n",
    "\n",
    "# define path to store dataset\n",
    "path = \"Datasets/mnist\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "def fetch(url):\n",
    "    if os.path.exists(path) is False:\n",
    "        os.makedirs(path)\n",
    "\n",
    "    fp = os.path.join(path, hashlib.md5(url.encode(\"utf-8\")).hexdigest())\n",
    "    if os.path.isfile(fp):\n",
    "        with open(fp, \"rb\") as f:\n",
    "            data = f.read()\n",
    "    else:\n",
    "        with open(fp, \"wb\") as f:\n",
    "            data = requests.get(url).content\n",
    "            f.write(data)\n",
    "    return np.frombuffer(gzip.decompress(data), dtype=np.uint8).copy()\n",
    "\n",
    "# load mnist dataset from yann.lecun.com, train data is of shape (60000, 28, 28) and targets are of shape (60000)\n",
    "train_data = fetch(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\")[0x10:].reshape((-1, 28, 28))\n",
    "train_targets = fetch(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\")[8:]\n",
    "test_data = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\")[0x10:].reshape((-1, 28, 28))\n",
    "test_targets = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\")[8:]\n",
    "\n",
    "# uncomment to show images from dataset using OpenCV\n",
    "# for train_image, train_target in zip(train_data, train_targets):\n",
    "#     train_image = cv2.resize(train_image, (400, 400))\n",
    "#     cv2.imshow(\"Image\", train_image)\n",
    "#     # if Q button break this loop\n",
    "#     if cv2.waitKey(0) & 0xFF == ord(\"q\"):\n",
    "#         break\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# define training hyperparameters\n",
    "n_epochs = 5\n",
    "batch_size_train = 64\n",
    "batch_size_test = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# reshape data to (items, channels, height, width) and normalize to [0, 1]\n",
    "train_data = np.expand_dims(train_data, axis=1) / 255.0\n",
    "test_data = np.expand_dims(test_data, axis=1) / 255.0\n",
    "\n",
    "# split data into batches of size [(batch_size, 1, 28, 28) ...]\n",
    "train_batches = [np.array(train_data[i:i+batch_size_train]) for i in range(0, len(train_data), batch_size_train)]\n",
    "# split targets into batches of size [(batch_size) ...]\n",
    "train_target_batches = [np.array(train_targets[i:i+batch_size_train]) for i in range(0, len(train_targets), batch_size_train)]\n",
    "\n",
    "test_batches = [np.array(test_data[i:i+batch_size_test]) for i in range(0, len(test_data), batch_size_test)]\n",
    "test_target_batches = [np.array(test_targets[i:i+batch_size_test]) for i in range(0, len(test_targets), batch_size_test)]\n",
    "\n",
    "# create model\n",
    "model = Net()\n",
    "\n",
    "# uncomment to print model summary\n",
    "#summary(model, (1, 28, 28), device=\"cpu\")\n",
    "\n",
    "# define loss function and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# create training loop\n",
    "def train(epoch):\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    loss_sum = 0\n",
    "    # create a progress bar\n",
    "    train_pbar = tqdm(zip(train_batches, train_target_batches), total=len(train_batches))\n",
    "    for index, (data, target) in enumerate(train_pbar, start=1):\n",
    "\n",
    "        # convert data to torch.FloatTensor\n",
    "        data = torch.from_numpy(data).float()\n",
    "        target = torch.from_numpy(target).long()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress bar with loss value\n",
    "        loss_sum += loss.item()\n",
    "        train_pbar.set_description(f\"Epoch {epoch}, loss: {loss_sum / index:.4f}\")\n",
    "\n",
    "# create testing loop\n",
    "def test(epoch):\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    correct, loss_sum = 0, 0\n",
    "    # create progress bar\n",
    "    val_pbar = tqdm(zip(test_batches, test_target_batches), total=len(test_batches))\n",
    "    with torch.no_grad():\n",
    "        for index, (data, target) in enumerate(val_pbar, start=1):\n",
    "\n",
    "            # convert data to torch.FloatTensor\n",
    "            data = torch.from_numpy(data).float()\n",
    "            target = torch.from_numpy(target).long()\n",
    "\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "\n",
    "            # update progress bar with loss and accuracy values\n",
    "            loss_sum += loss_function(output, target).item() / target.size(0)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum() / target.size(0)\n",
    "\n",
    "            val_pbar.set_description(f\"val_loss: {loss_sum / index:.4f}, val_accuracy: {correct / index:.4f}\")\n",
    "\n",
    "\n",
    "# train and test the model\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "# define output path and create folder if not exists\n",
    "output_path = \"Models/06_pytorch_introduction\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# save model.pt to defined output path\n",
    "torch.save(model.state_dict(), os.path.join(output_path, \"model.pt\"))\n",
    "\n",
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import os\n",
    "\n",
    "assert os.path.exists('hw07_data_dict.npy'), 'Please, download `hw07_data_dict.npy` and place it in the working directory'\n",
    "\n",
    "def get_predictions(model, eval_data, step=10):\n",
    "    \n",
    "    predicted_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, len(eval_data), step):\n",
    "            y_predicted = model(eval_data[idx:idx+step].reshape(-1, 784))\n",
    "            predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "    \n",
    "    predicted_labels = torch.cat(predicted_labels)\n",
    "    return predicted_labels\n",
    "\n",
    "loaded_data_dict = np.load('hw07_data_dict.npy', allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [10, 784]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mg:\\VSCode\\yandex_practicum\\assignment 07\\assignment_mnist.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/VSCode/yandex_practicum/assignment%2007/assignment_mnist.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m predicted_labels\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/VSCode/yandex_practicum/assignment%2007/assignment_mnist.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loaded_data_dict \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mhw07_data_dict.npy\u001b[39m\u001b[39m'\u001b[39m, allow_pickle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/VSCode/yandex_practicum/assignment%2007/assignment_mnist.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m submission_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/VSCode/yandex_practicum/assignment%2007/assignment_mnist.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: get_predictions(model, torch\u001b[39m.\u001b[39;49mFloatTensor(loaded_data_dict\u001b[39m.\u001b[39;49mitem()[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m]))\u001b[39m.\u001b[39mnumpy(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/VSCode/yandex_practicum/assignment%2007/assignment_mnist.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m: get_predictions(model, torch\u001b[39m.\u001b[39mFloatTensor(loaded_data_dict\u001b[39m.\u001b[39mitem()[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]))\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/VSCode/yandex_practicum/assignment%2007/assignment_mnist.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/VSCode/yandex_practicum/assignment%2007/assignment_mnist.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39msubmission_dict_hw07.npy\u001b[39m\u001b[39m'\u001b[39m, submission_dict, allow_pickle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/VSCode/yandex_practicum/assignment%2007/assignment_mnist.ipynb#X26sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFile saved to `submission_dict_hw07.npy`\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mg:\\VSCode\\yandex_practicum\\assignment 07\\assignment_mnist.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/VSCode/yandex_practicum/assignment%2007/assignment_mnist.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/VSCode/yandex_practicum/assignment%2007/assignment_mnist.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(eval_data), step):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/VSCode/yandex_practicum/assignment%2007/assignment_mnist.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         y_predicted \u001b[39m=\u001b[39m model(eval_data[idx:idx\u001b[39m+\u001b[39;49mstep]\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m784\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/VSCode/yandex_practicum/assignment%2007/assignment_mnist.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         predicted_labels\u001b[39m.\u001b[39mappend(y_predicted\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/VSCode/yandex_practicum/assignment%2007/assignment_mnist.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m predicted_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(predicted_labels)\n",
      "File \u001b[1;32mg:\\VSCode\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mg:\\VSCode\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1527\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1528\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1530\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1531\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mg:\\VSCode\\yandex_practicum\\assignment 07\\model.py:15\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(F\u001b[39m.\u001b[39mmax_pool2d(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x), \u001b[39m2\u001b[39m))\n\u001b[0;32m     16\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(F\u001b[39m.\u001b[39mmax_pool2d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2_drop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)), \u001b[39m2\u001b[39m))\n\u001b[0;32m     17\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m320\u001b[39m)\n",
      "File \u001b[1;32mg:\\VSCode\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mg:\\VSCode\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1527\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1528\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1530\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1531\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mg:\\VSCode\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mg:\\VSCode\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [10, 784]"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import os\n",
    "\n",
    "assert os.path.exists('hw07_data_dict.npy'), 'Please, download `hw07_data_dict.npy` and place it in the working directory'\n",
    "\n",
    "def get_predictions(model, eval_data, step=10):\n",
    "    \n",
    "    predicted_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, len(eval_data), step):\n",
    "            y_predicted = model(eval_data[idx:idx+step].reshape(-1, 784))\n",
    "            predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "    \n",
    "    predicted_labels = torch.cat(predicted_labels)\n",
    "    return predicted_labels\n",
    "\n",
    "loaded_data_dict = np.load('hw07_data_dict.npy', allow_pickle=True)\n",
    "\n",
    "submission_dict = {\n",
    "    'train': get_predictions(model, torch.FloatTensor(loaded_data_dict.item()['train'])).numpy(),\n",
    "    'test': get_predictions(model, torch.FloatTensor(loaded_data_dict.item()['test'])).numpy()\n",
    "}\n",
    "\n",
    "np.save('submission_dict_hw07.npy', submission_dict, allow_pickle=True)\n",
    "print('File saved to `submission_dict_hw07.npy`')\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
